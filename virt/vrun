#!/bin/sh


#
# For VM:
# Using cloud-hypervisor initially - for more diversity, qemu should also work, crosvm would be ideal 
# if it had downloadable binaries.
#
# Using virtiofs to access shared files with the host filesystem. A local disk is usually required as cache and to
# keep Java and other apps happy.
# 
# The rootfs can be a host dir (similar to containers) or the immutable recovery image.

# Directory layout outside the container:
#    WORK ( defaults to /x/vol/$NAME )
#    SRCDIR - /ws/$NAME - source directory
# 
# Directory layout inside the container:
#   /ws/$APP - the source directory (read only)
#   /work/$APP - the work directory, with /work/$APP/dist for the files to be distributed
#   /run/secrets/$APP - secrets volume
#
# Various cache directories set to files in /work/$APP/cache, etc


#set -x
set -e

if [ -f ${HOME}/.config/vrun/env ]; then 
  source ${HOME}/.config/vrun/env
fi

export SRCDIR=${SRCDIR:-$(pwd)}

# Defaults to the name of the working directory.
# This is the application name (may have multiple pod).
POD=${POD:-$(basename $PWD)}

VM=${VM:-${POD}}
NAME=${NAME:-$POD}
APP=${APP:-${POD}}

# If the script is named vrun-devvm -> the VM will be devvm
# Otherwise the current working dir will be used as VM dir. 
# Images are still in /x/vol/images 
#if [ -z ${VM} ]; then
#   vmsuffix="${0#*vrun-}"

#   if [ -z "$vmsuffix" ]; then
#  VM=$(basename $PWD)
  # else
  #   VM=$vmsuffix
  # fi
#fi

if [ -z ${POD} ]; then 
  echo "POD must be set to the target container or VM name"
  exit 1
fi

# If set, will be added as a prefix to the command
#TIME="time "
TIME=""

# Base directory for all VM files.
VOLS=${VOLS:-/x/vol}

# Directory with hypervisor binaries, kernel, initrd, recovery
VIRT=${VIRT:-/x/opt/virt}
IMGDIR=${IMGDIR:-/x/vol/images}


WORK=${WORK:-${VOLS}/${POD}}

# IMAGE and other parameters loaded from this file
if [ -f ${WORK}/env ]; then 
  . ${WORK}/env
fi

### Hypvervisor

# For each pod, start a single Virtiofsd - should be mounted as root (/).
# Use bind mounts to manage the content.
run_virtiofsd() {
  local base=${1:-${WORK}}

  mkdir -p $base/share/lib/modules
  if [ -d ${base}/rootfs ]; then 
    dir=${base}/rootfs
  else
    mkdir -p ${base}/share
    dir=${base}/share
  fi

  if [ -f ${WORK}/virtiofsd.pid ]; then 
    kill -9 $(cat ${WORK}/virtiofsd.pid) || true
    rm -f ${WORK}/virtiofsd.pid
  fi

  rm -f ${WORK}/virtiofsd.sock || true

  # Faster than virtio-9p or regular 9p/nfs
  # FUSE messages over vhost-user socket
  # DAX (using host buffers) not enabled.
  #       --log-level debug 
  ${VIRT}/virtiofsd \
      --socket-path=${WORK}/virtiofs.sock \
      --shared-dir=${dir} \
      --cache=never \
      --xattr \
      --allow-direct-io \
      --allow-mmap \
      --sandbox chroot \
      --thread-pool-size=2 > ${WORK}/virtiofs.log  2>&1 &
      
    # Default sandbox is namespace, doesn't work in chroot  
    # --allow-mmap should only be used with exclusive access
    #  --log-level debug \
    # 

    # Save the PID
    echo $! > ${WORK}/virtiofsd.pid

    return 0
}

# Testing: 
# - DEBUG=init
#    - Run a command before swaproot, poweroff immediatly after.
#      Useful for finding startup time.
#          `CMD_OPTS=cmdx=/bin/ls`
# - Run a command after pivot, poweroff after
#      cmd="df" 

# Alpine kernel includes no filesystem - virtiofs is a module - so modules must be loaded
# by the initrd. Debian has similar issues.

vm_init() {
  mount -o bind /lib/modules ${WORK}/share/lib/modules
  # TODO: additional bind mounts configured in the lib
}

# Start will run run the pod (VM) in background, like a container.
# 
# Few env variables impact the startup:
#  - DEBUG=init - will run a shell in the initrd
#  - 'cmd' - will run a command in the VM after startup in the rootfs, foreground.
start() {
  BASE=2

  OPTS="$OPTS --pvpanic"
  OPTS="$OPTS --rng src=/dev/urandom"
  OPTS="$OPTS --api-socket=${WORK}/ch.sock"
  OPTS="$OPTS --memory size=${vm_mem},shared=on --cpus boot=${vm_cpu}"
  OPTS="$OPTS --balloon size=1G,deflate_on_oom=on "

  # Networking - add the tap with specific interface, add cmd line for the guest side.
  #local hostip=$(_ip_base $BASE)
  #local guestip=$(_ip_base $(( $BASE + 1 )) )
  

  # virtio-net is used for eth
  # tap is the address of the tap on the VM side
  # Adding ip=172.17.1.22 to kernel cmdline adds 10 sec delay
  #OPTS="$OPTS --net tap=vm-$VM,ip=${hostip},mask=255.255.128.0,iommu=on"
  #CMD_OPTS="$CMD_OPTS net.ifnames=0 initos.ip=${guestip} initos.gw=${hostip}"
  OPTS="$OPTS --net tap=vm-$VM,iommu=on"
  CMD_OPTS="$CMD_OPTS initos.host=${POD}"
  CMD_OPTS="$CMD_OPTS net.ifnames=0 initos.ip=${vm_ip} initos.gw=${vm_gw}"
  
  OPTS="$OPTS --kernel ${VIRT}/vmlinuz"
  OPTS="$OPTS --initramfs ${VIRT}/initramfs "

  CMD_OPTS="$CMD_OPTS trace_clock=global panic=1  panic_on_oops=1 reboot=acpi "
  
  # Using the rootfs as root or shared
  run_virtiofsd ${WORK}  
  OPTS="$OPTS --fs tag=/dev/root,socket=${WORK}/virtiofs.sock,num_queues=1,queue_size=512"

  DISKS=
  # TODO: use an env variable to configure the rootfs image.
  # TODO: Allow read only rootfs.
  if [ -f ${IMGDIR}/${VM}-rootfs.img ] ; then
      DISKS="--disk path=${IMGDIR}/${VM}-rootfs.img path=${IMGDIR}/${VM}.img"
  elif [ -f ${IMGDIR}/${VM}.img ] ; then
      DISKS="--disk path=${IMGDIR}/${VM}.img"
  fi
  if [ -f ${IMGDIR}/${VM}-data.img ]; then
    DISKS="$DISKS path=${IMGDIR}/${VM}-data.img" 
  fi
  OPTS="$OPTS $DISKS"
    
  #  --vsock \
  #  --pmem file=${WORK}/boot/efi/recovery.sqfs \

  rm -f ${WORK}/ch.sock
  rm -f ${WORK}/serial.socket
 
  #set -x

  mkdir -p ${WORK}/share/lib/modules
  mount -o bind /initos/rootfs/lib/modules ${WORK}/share/lib/modules

  if [ "${DEBUG}" = "init" ]; then
    # Debian kernel doesn't seem to work with hvc0, serial is fine
    # Verbose
    OPTS="$OPTS --serial tty --console off"
    CMD_OPTS="$CMD_OPTS debug loglevel=8 ignore_loglevel"
    CMD_OPTS="$CMD_OPTS console=hvc0 console=ttyS0 "
    CMD_OPTS="$CMD_OPTS systemd.log_level=debug systemd.log_target=console"
    CMD_OPTS="$CMD_OPTS systemd.debug_shell"
    # --serial tty --console off --cmdline "console=ttyS0" results in ttyS0 as the console
    # Otherwise the virtio-console driver is used - faster but less early messages

    if [ -n "${cmd}" ] ; then 
      # Instead of start - get a shell
      $TIME ${VIRT}/cloud-hypervisor $OPTS \
      --cmdline "${CMD_OPTS} --${cmd}"
    else 
      $TIME ${VIRT}/cloud-hypervisor $OPTS \
      --cmdline "${CMD_OPTS} initos.cmd1=/bin/sh"
    fi 
  elif [ -n "${cmd}" ] ; then 
    
    OPTS="$OPTS --serial tty --console off"
    CMD_OPTS="$CMD_OPTS quiet console=ttyS0 "
    
    # Instead of start - get a shell (post chroot)
    $TIME ${VIRT}/cloud-hypervisor $OPTS \
       --cmdline "${CMD_OPTS} --${cmd}"
  elif [ "${FG}" = "1" ]; then
    OPTS="$OPTS --serial tty --console off"
    CMD_OPTS="$CMD_OPTS quiet console=ttyS0 "
  
    ${VIRT}/cloud-hypervisor $OPTS --cmdline "${CMD_OPTS}"
  else
    OPTS="$OPTS --serial socket=${WORK}/serial.socket --console off"
    CMD_OPTS="$CMD_OPTS quiet"
    CMD_OPTS="$CMD_OPTS console=ttyS0 "
  
    ${VIRT}/cloud-hypervisor $OPTS --cmdline "${CMD_OPTS}" &
    echo $! > ${WORK}/vm.pid
  fi

  # --net .. mac=...
  #systemd.unit=rescue.target systemd.debug-shell=1 

  # exit with openrc-shutdown -p 0
  # or poweroff -d 0 -f
}

# Each VM is expected to have a SSH server. Just SSH and run the command.
# This is the same as 'remote pods'.
run_vm() {
  ssh $VMIP "$@"
}

stop_vm() {
  vm_ctrl shutdown-vmm
}

vm_console() {
  socat UNIX-CONNECT:${WORK}/serial.socket  -
}

# Expose ch-remote control
vm_ctrl() {
  ${VIRT}/ch-remote --api-socket ${WORK}/ch.sock "$@"
}

CMD=$1
shift
$CMD "$@"
