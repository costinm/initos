#!/bin/sh

# Build script for initos. The build typically runs 
# inside a container with optional volumes for cache.
#
# The 'sign' function is the entrypoint for the container, will
# generate a signed efi partition including the sqfs signed image
# and patching.
# 
# The results:
# - creating a 'base-recovery' image, that can be run in Docker in a trusted machine to generate signed
# artifacts including bootstrap ssh authorized keys and user-specific roots of trust.
#
# - creation of a directory containing all artifacts needed for install by copying to an existing
# USB EFI partition.
#

set -x
set -e

# Destination dir for the efi partition artifacts.
# May include additional files to bake into images.
# Signing the images also requires a volume holding the signing keys
# and configs.
WORK=${WORK:-/data}

mkdir -p ${WORK}/efi/EFI/BOOT /opt/virt


# Build the initramfs for the real kernel
# Expectes /boot, /lib/modules, /lib/firmware to have the right
# mounts and /boot/version to hold the desired kernel version.
# 
# This runs in an alpine container (for now) - the init will be based
# on the lighter musl binaries.
build_initrd() {
  local VER=${1:-$(cat /boot/version)}
  local img=${2:-/boot/initos-initrd.img}

  [ ! -f /lib/modules/${VER}/modules.dep ] && echo "Missing modules" && return

    # Uses mkinitfs.conf added from file in the docker image.
    # Other options: virtio, squashfs, 9p, raid, cdrom, lvm, ext4, kms (video)

    # -k - keep the file
    # -t - temp dir where all is stored
    # -i - what init file to use - replace default with our script.
    #rm -rf ${WORK}/initrd
    # -k -t ${WORK}/initrd

    # Not loaded: btrfs, tpm2

    # Technically cryptsetup, tpm could be loaded from recovery
    # image, with a double pivot.
    cp /etc/passwd /usr/share/mkinitfs/passwd
    cp /etc/group /usr/share/mkinitfs/group

    #echo "features=\"ata base keymap mmc nvme scsi usb cryptsetup squashfs initos eudev\"" > /etc/mkinitfs/mkinitfs.conf
    
    # clevis ? 
    #rm -rf ${WORK}/initfs.host
    # -t ${WORK}/initfs.host
    mkinitfs -i /sbin/initos-initrd \
      -F "ata base keymap mmc nvme scsi usb cryptsetup squashfs initos eudev" \
      -o ${img} $VER

    # /init will be the inito-initrd

    # Add -k to keep and inspect, or extract it
    # Currently 90M, 54M in modules.
}


# Generate an unsigned efi image, based on alpine with a custom init script.
# alpine_initrd is run in an alpine container, rescue or chroot.
#
# The init image will be generated in /boot/initramfs.
# Modules are expected to be in /lib/modules/modules-KERNEL_VERSION
# Kernel is expected to be in /boot/vmlinux-KERNEL_VERSION
#
# For secure mode, kernel+initrd+cmdline are signed, but the rootfs needs to
# be either signed or on a LUKS partition ( where encryption+signing happens as well).
#
# In the first case, normally the SHA of the rootfs needs to be passed as an argument
# either in kernel cmdline or as a file in initrd.
#
# The '@firmware' and '@modules' subvolumes must also be created
# if using separate volumes - for now they're in @recovery rootfs.

# Add the current docker image to an existing SQFS file.
# This is useful if the kernel+modules+firmware is created independently.
sqfs() {
  local DIR=${1:-/data/efi/initos}
  local name=${2:-initos}

  # -one-file-system also works on the host - but not so well in a container.

  # buildah unshare -m A=debui -- \
  #  sh -c 'tar -cf - -C ${A} .' | \
  #   sqfstar ${WORK}/vimg/virt.sqfs -e .dockerenv

  #alpine_clear

  # Excluding boot from the sqfs (kernel remains on the image and is used to build
  # the EFI, but no need to add a copy)
  # However the cloud kernel is needed for VM

  mkdir -p ${DIR}
  rm -f ${DIR}/${name}.*

  cd /

  # in case they are still around
  rm -rf /etc/uefi-keys 
  
  # Doesn't include /boot files (kernel is separate)
  # Includes both normal and cloud modules
  mksquashfs . ${DIR}/${name}.sqfs \
     -regex \
     -e "x/.*" \
     -e "etc/uefi-keys" \
     -e ".dockerenv" \
     -e "boot/.*" \
     -e "data/.*" \
     -e "proc/.*" \
     -e "sys/.*" \
     -e "run/.*" \
     -e "work/.*" \
     -e "ws/.*" \
     -e "tmp/.*" \
     -e "var/cache/apt/.*" \
     -e "etc/apk/cache.*"
    
  echo "Created ${DIR}/${name}.sqfs"
}

alpine_clear() {
  rm /var/lib/dbus/machine-id || true
  rm /etc/hostname || true
  rm /etc/resolv.conf || true
  echo "" > /etc/machine-id || true
}

export DEBIAN_FRONTEND=noninteractive
APTINSTALL="apt install --assume-yes --no-install-recommends "

debian_rootfs() {
  add_deb_kernel
  
  add_deb_core

  add_deb_kernel_nvidia
  
  save_lib
  sqfs
}

debian_rootfs_base() {
  add_deb_kernel
  
  add_deb_core

  add_deb_kernel_nvidia
}

add_deb_core() {
  $APTINSTALL ca-certificates curl gpg  \
     linux-headers-$(cat /boot/version) 


  $APTINSTALL  tpm2-tools  \
     tini  bsdutils dosfstools fdisk \
    gdisk hdparm file findutils fuse3 btrfs-progs lsof \
    \
    rsync wpasupplicant ifupdown ifupdown-extra \
     wireless-tools bridge-utils net-tools tcpdump iptables iproute2  \
    nftables iperf3 openssh-server  iw \
    \
    cryptsetup efibootmgr squashfs-tools 

}

# Adds Kernel, firmware, Nvidia driver.
# Builds the initrd images for intel/amd uCode.
# This adds X
# 
# Since nvidia also has libraries and docker/podman support - will use 
# this debian image as the 'sidecar OS', so including more packages.
add_deb_kernel_nvidia() {


  # Will use systemd or script - may start it as 'main' as well, without 
  # an actual sidecar.


  # Not the current kernel on the builder machine
 

  curl -fsSL -o /tmp/cuda.deb https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.1-1_all.deb
  dpkg -i /tmp/cuda.deb
  curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |  gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg 
  
  curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    
  apt update
  
  # The debian kernel (old) can be instlled with:
  #$APTINSTALL nvidia-driver 

  # This has systemd as a dependency...
  $APTINSTALL cuda-drivers
  $APTINSTALL nvidia-container-toolkit  
 
}

# Save_lib is used in the debian image to save the kernel and modules on 
# /data, to be used by the 'sign' image.
#
# The other choice is to have a large 'sign' image that bundles the kernel
# and modules and is updated with every kernel release. 
# Since one way or another we need /data and /config for signing, we may 
# as well use a separate step of creating the boot/ and lib/modules which
# don't change except on new kernel releases.
save_lib() {
  mkdir -p /data/lib /data/boot
  cp -a /boot/* /data/boot
  cp -a /lib/modules /data/lib
  cp -a /lib/firmware /data/lib
}

# Adds Kernel, firmware
# Builds the initrd images for intel/amd uCode.
add_deb_kernel() {
  echo deb http://deb.debian.org/debian/ bookworm main contrib non-free non-free-firmware >> /etc/apt/sources.list

  apt update
  export INITRD=No
  apt install -y --no-install-recommends \
   linux-image-amd64 firmware-misc-nonfree \
     firmware-linux-free \
       firmware-realtek \
       firmware-iwlwifi firmware-atheros \
       amd64-microcode intel-microcode

  TMPDIR=/tmp/initrd
  rm -rf $TMPDIR
  mkdir $TMPDIR

  cd $TMPDIR
  mkdir -p kernel/x86/microcode

  if [ -d /lib/firmware/amd-ucode ]; then
    cat /lib/firmware/amd-ucode/microcode_amd*.bin > kernel/x86/microcode/AuthenticAMD.bin
  fi
  find . | cpio -o -H newc >/boot/amd-ucode.img
  
  rm -rf kernel
  mkdir -p kernel/x86/microcode
  if [ -d /lib/firmware/intel-ucode ]; then
   cat /lib/firmware/intel-ucode/* > kernel/x86/microcode/GenuineIntel.bin
  fi

  find . | cpio -o -H newc >/boot/intel-ucode.img

  ver=$(ls /lib/modules)
  echo -n ${ver} > /boot/version

  # Also install the cloud kernel - upgrade them at the same cadence, and
  # will be used to run the real root as a VMs for more security.
  # Images grows from 795 -> 909 M, Initos image from 568-> 634
  apt install -y --no-install-recommends \
     linux-image-cloud-amd64

  # Can't stop the creation - but not used. Just need the kernel and modules.
  rm -rf /boot/initrd.img* /tpm/* /tmp/* || true
}

add_virt_kernel() {
  apt update
  export INITRD=No
  apt install -y --no-install-recommends \
     linux-image-cloud-amd64
  ver=$(ls /lib/modules)
  echo -n ${ver} > /boot/versionv
  # Can't stop the creation - but not used. Just need the kernel and modules.
  rm -rf /boot/initrd.img*  || true
}


# vinit updates the initram-Fs for the virt kernel
# Expectes /boot, /lib/modules, /lib/firmware to have the right
# mounts and /boot/version-virt to hold the desired kernel version.
# 
# This runs in an alpine container (for now) - the init will be based
# on the ligher musl binaries.
vinit() {
  VER=$(ls /lib/modules | grep cloud | tail)

  mkdir -p /data/virt

  cp /boot/vmlinuz-${VER} /data/virt/vmlinuz
  # btrfs is pretty slow for a VM - the rootfs may have all the modules
  # and load it as needed.
  # squashfs may not be needed if modules/firmware are on the ext4 rootfs
  # or on virtio.
  mkinitfs -k  -i /sbin/initos-initrd-vm \
     -F "base btrfs virtio initosvm" \
     -o /data/virt/initramfs ${VER}

}

vinitl() {
  VER=$(ls /lib/modules | grep cloud | tail)
  mkinitfs -k  -i /sbin/initos-initrd-vm \
     -F "base btrfs virtio initosvm" \
     -o /opt/virt/initramfs ${VER}
}

setup_initrd() {
  build_initrd 
  vinit
}

# debootstrap_kernel() {
#   mkdir /tmp/deb
#   debootstrap --variant=minbase --print-debs sid  /tmp/deb
# }

"$@"
