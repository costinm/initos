#!/bin/sh

# Build script for a debian rootfs.
# Kernel, Nvidia modules and basic utils.

set -x
set -e

# Destination dir for the efi partition artifacts.
# May include additional files to bake into images.
# Signing the images also requires a volume holding the signing keys
# and configs.
WORK=${WORK:-/data}

# Save the kernel and modules on /data, to be used by the 'sign' image and
# to build the initrd.
# Dockerfile is bind-mounting from the kernel stage - with buildah it is faster
# to copy (at least on btrfs, COW) - it can't mount from a container and creating
# an image involves tar, so no longer COW.
#
# The other choice is to have a large 'sign' image that bundles the kernel
# and modules and is updated with every kernel release. 
# Since one way or another we need /data and /config for signing, we may 
# as well use a separate step of creating the boot/ and lib/modules which
# don't change except on new kernel releases.
save_boot() {
  mkdir -p /data/lib /data/boot
  cp -a /boot/* /data/boot
  cp -a /lib/modules /data/lib
  cp -a /lib/firmware /data/lib
}


# Creat a sqfs under $1 with the name $2, containing all files in this 
# container. 
#
# Another way is to export (pull and export or using crane) and create 
# the sqfs externally.
sqfs() {
  local DIR=${1:-/data/efi/initos}
  local name=${2:-initos}

  # -one-file-system also works on the host - but not so well in a container.

  # buildah unshare -m A=debui -- \
  #  sh -c 'tar -cf - -C ${A} .' | \
  #   sqfstar ${WORK}/vimg/virt.sqfs -e .dockerenv

  #alpine_clear

  # Excluding boot from the sqfs (kernel remains on the image and is used to build
  # the EFI, but no need to add a copy)
  # However the cloud kernel is needed for VM

  mkdir -p ${DIR}
  rm -f ${DIR}/${name}.*

  cd /

  # Doesn't include /boot files (kernel is separate)
  # Includes both normal and cloud modules
  mksquashfs . ${DIR}/${name}.sqfs \
     -regex \
     -e "x/.*" \
     -e "etc/uefi-keys" \
     -e ".dockerenv" \
     -e "data/.*" \
     -e "proc/.*" \
     -e "sys/.*" \
     -e "run/.*" \
     -e "tmp/.*" \
     -e "var/cache/apt/.*" \
     -e "etc/apk/cache.*"
    
  # Not excluding boot, lib - work and ws are no longer mounted (old versions)

  echo "Created ${DIR}/${name}.sqfs"
}


export DEBIAN_FRONTEND=noninteractive
APTINSTALL="apt install --assume-yes --no-install-recommends "


debian_rootfs_base() {
  add_deb_kernel
  
  add_deb_core

  # No longer installing the nvidia driver - should be in a 
  # VM, pcie delegation works pretty well and better to isolate it.
  ### add_deb_kernel_nvidia
}

add_deb_core() {
  $APTINSTALL ca-certificates curl gpg  


  $APTINSTALL  tpm2-tools  \
     tini  bsdutils dosfstools fdisk \
    gdisk hdparm file findutils fuse3 btrfs-progs lsof \
    \
    rsync wpasupplicant ifupdown ifupdown-extra \
     wireless-tools bridge-utils net-tools tcpdump iptables iproute2  \
    nftables iperf3 openssh-server  iw \
    \
    cryptsetup efibootmgr squashfs-tools pciutils

  #systemctl enable ssh.service
}

# Adds Kernel, firmware, Nvidia driver.
# Builds the initrd images for intel/amd uCode.
# This adds X
# 
# Since nvidia also has libraries and docker/podman support - will use 
# this debian image as the 'sidecar OS', so including more packages.
add_deb_kernel_nvidia() {


  # Will use systemd or script - may start it as 'main' as well, without 
  # an actual sidecar.


  # Not the current kernel on the builder machine
 

  curl -fsSL -o /tmp/cuda.deb https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.1-1_all.deb
  dpkg -i /tmp/cuda.deb
  if [ ! -f /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg ] ; then
     curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |  gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg 
    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
      sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
      tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
     apt update
  fi
    
  
  # The debian kernel (old) can be instlled with:
  #$APTINSTALL nvidia-driver 

  # This has systemd as a dependency...
  $APTINSTALL cuda-drivers linux-headers-$(cat /boot/version) 
  $APTINSTALL nvidia-container-toolkit  
 
  /usr/lib/dkms/dkms_autoinstaller start $(cat /boot/version )
  dkms status
}

# Adds Kernel, firmware
# Builds the initrd images for intel/amd uCode.
add_deb_kernel() {
  echo deb http://deb.debian.org/debian/ bookworm main contrib non-free non-free-firmware >> /etc/apt/sources.list

  apt update
  export INITRD=No
  $APTINSTALL \
   linux-image-amd64 firmware-misc-nonfree \
     firmware-linux-free \
       firmware-realtek \
       firmware-iwlwifi firmware-atheros \
       amd64-microcode intel-microcode

  TMPDIR=/tmp/initrd
  rm -rf $TMPDIR
  mkdir $TMPDIR

  cd $TMPDIR
  mkdir -p kernel/x86/microcode

  if [ -d /lib/firmware/amd-ucode ]; then
    cat /lib/firmware/amd-ucode/microcode_amd*.bin > kernel/x86/microcode/AuthenticAMD.bin
  fi
  find . | cpio -o -H newc >/boot/amd-ucode.img
  
  rm -rf kernel
  mkdir -p kernel/x86/microcode
  if [ -d /lib/firmware/intel-ucode ]; then
   cat /lib/firmware/intel-ucode/* > kernel/x86/microcode/GenuineIntel.bin
  fi

  find . | cpio -o -H newc >/boot/intel-ucode.img

  ver=$(ls /lib/modules |grep -v cloud)
  echo -n ${ver} > /boot/version

  # No longer doing this - separate builder/image for VMs. 
  # Also install the cloud kernel - upgrade them at the same cadence, and
  # will be used to run the real root as a VMs for more security.
  # Images grows from 795 -> 909 M, Initos image from 568-> 634
  # $APTINSTALL \
  #    linux-image-cloud-amd64
  # ver=$(ls /lib/modules |grep cloud)
  # echo -n ${ver} > /boot/cloud-version

  # Can't stop the creation - but not used. Just need the kernel and modules.
  rm -rf /boot/initrd.img* /tpm/* /tmp/* || true
}


"$@"
